{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOabTZgVKNOGWmnwzp+78I7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SalmanMonir9/NVIDIA-Rapid-Application-Development-with-Large-Language-Models-LLMs-/blob/main/NVDIA_Test_Rapid_Application_Development_using_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8es5zJtvHRp"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def ask_about_image(image_path: str, question: str = None, vlm_llm=None) -> str:\n",
        "    \"\"\"\n",
        "    Uses a VLM to describe an image.\n",
        "    Args:\n",
        "        image_path: Path to the image file.\n",
        "        question: Optional question to ask about the image. If None, asks for a detailed description.\n",
        "        vlm_llm: An initialized LangChain ChatNVIDIA (or similar) instance capable of vision.\n",
        "    Returns:\n",
        "        A text description of the image.\n",
        "    \"\"\"\n",
        "    if vlm_llm is None:\n",
        "        raise ValueError(\"A VLM LLM instance must be provided.\")\n",
        "\n",
        "    # Encode image to base64\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        image_bytes = f.read()\n",
        "        base64_image = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "    from langchain_core.messages import HumanMessage\n",
        "\n",
        "    if question is None:\n",
        "        # Default instruction for description\n",
        "        content_parts = [\n",
        "            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"},\n",
        "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
        "        ]\n",
        "    else:\n",
        "        # Use the specific question\n",
        "        content_parts = [\n",
        "            {\"type\": \"text\", \"text\": question},\n",
        "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
        "        ]\n",
        "\n",
        "    messages = [HumanMessage(content=content_parts)]\n",
        "\n",
        "    # Invoke the VLM\n",
        "    response = vlm_llm.invoke(messages)\n",
        "    return response.content\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def generate_images(prompt: str, n: int = 1):\n",
        "    ####################################################################\n",
        "    ## < EXERCISE SCOPE\n",
        "\n",
        "    # Load the pipeline (using Stable Diffusion XL for better quality)\n",
        "    pipe = DiffusionPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.float16,\n",
        "        variant=\"fp16\",\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    # Enable memory optimizations if needed\n",
        "    pipe.enable_model_cpu_offload()\n",
        "\n",
        "    # Generate images\n",
        "    results = pipe(\n",
        "        prompt=prompt,\n",
        "        num_images_per_prompt=n,\n",
        "        num_inference_steps=30,  # Higher = better quality but slower\n",
        "        guidance_scale=7.5,      # Controls prompt adherence\n",
        "    )\n",
        "\n",
        "    # Save images and return paths\n",
        "    image_paths = []\n",
        "    os.makedirs(\"generated_images\", exist_ok=True)\n",
        "\n",
        "    for i, image in enumerate(results.images):\n",
        "        path = f\"generated_images/output_{i}.png\"\n",
        "        image.save(path)\n",
        "        image_paths.append(path)\n",
        "\n",
        "    return image_paths\n",
        "\n",
        "    ## EXERCISE SCOPE >\n",
        "    ####################################################################\n",
        "\n",
        "def plot_imgs(image_paths, r=2, c=2):\n",
        "    fig, axes = plt.subplots(r, c)\n",
        "    for i, ax in enumerate(getattr(axes, \"flat\", [axes])):\n",
        "        img = plt.imread(image_paths[i])\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "description = \"A futuristic AI agent with holographic interfaces, glowing blue accents, and sleek metallic design\"\n",
        "images = generate_images(description, n=1)\n",
        "plot_imgs(images, 1, 1)"
      ],
      "metadata": {
        "id": "zgTio1hivJw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Initialize SDXL pipeline\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True\n",
        ")\n",
        "pipe = pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def generate_images(prompt: str, n: int = 1) -> list:\n",
        "    \"\"\"Generate images from prompt with error handling\"\"\"\n",
        "    try:\n",
        "        result = pipe(prompt, num_images_per_prompt=n)\n",
        "        return result.images\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating image: {e}\")\n",
        "        # Return blank images as fallback\n",
        "        return [Image.new('RGB', (512, 512), color='gray') for _ in range(n)]\n",
        "\n",
        "def plot_imgs(images, cols=2):\n",
        "    \"\"\"Display images in grid\"\"\"\n",
        "    if not images:\n",
        "        print(\"No images to display\")\n",
        "        return\n",
        "\n",
        "    rows = (len(images) + cols - 1) // cols\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
        "    axes = axes.ravel() if len(images) > 1 else [axes]\n",
        "\n",
        "    for ax, img in zip(axes, images):\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example prompts (replace with your actual prompt generation)\n",
        "new_sd_prompts = [\n",
        "    \"cyberpunk city at night with neon lights, rain effects, photorealistic, 8k ultra-detailed\",\n",
        "    \"cyberpunk city at night with neon lights, anime style, vibrant colors\",\n",
        "    \"cyberpunk city at night with neon lights, oil painting style\"\n",
        "]"
      ],
      "metadata": {
        "id": "BIHLJxZovOGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Initialize models\n",
        "sd_pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True\n",
        ")\n",
        "sd_pipe.to(\"cuda\") if torch.cuda.is_available() else sd_pipe.enable_model_cpu_offload()\n",
        "\n",
        "def plot_imgs(images, cols=2):\n",
        "    rows = (len(images) // cols + (1 if len(images) % cols else 0))\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(10, 5*rows))\n",
        "    for i, ax in enumerate(axes.flat if rows > 1 else [axes]):\n",
        "        if i < len(images):\n",
        "            ax.imshow(images[i])\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def ask_about_image(image_url: str) -> str:\n",
        "    \"\"\"Get image description from VLM\"\"\"\n",
        "    try:\n",
        "        # Simulate VLM output - replace with actual implementation\n",
        "        if \"agent\" in image_url:\n",
        "            return \"An AI agent interface with dashboard elements and analytics\"\n",
        "        elif \"multimodal\" in image_url:\n",
        "            return \"Diagram showing multimodal AI architecture\"\n",
        "        elif \"frog\" in image_url:\n",
        "            return \"A colorful tree frog sitting on a green leaf\"\n",
        "        elif \"cat\" in image_url:\n",
        "            return \"A cat painted in abstract style with vibrant colors\"\n",
        "        return \"An interesting image containing various elements\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error describing image: {e}\")\n",
        "        return \"An image with various elements\"\n",
        "\n",
        "def generate_prompts(description: str, n: int = 4) -> list[str]:\n",
        "    \"\"\"Generate optimized prompts using LLM\"\"\"\n",
        "    prompt_template = ChatPromptTemplate.from_template(\n",
        "        \"Generate {n} distinct Stable Diffusion prompts based on: {description}. \"\n",
        "        \"Each must include: 1) Main subject 2) Style/medium 3) Composition 4) Quality modifiers. \"\n",
        "        \"Return one prompt per line, no numbering.\"\n",
        "    )\n",
        "\n",
        "    chain = (\n",
        "        {\"n\": RunnablePassthrough(), \"description\": RunnablePassthrough()}\n",
        "        | prompt_template\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = chain.invoke({\"n\": n, \"description\": description})\n",
        "        return [p.strip() for p in response.split(\"\\n\") if p.strip()][:n]\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating prompts: {e}\")\n",
        "        base = f\"{description}, high quality, detailed, 4k\"\n",
        "        return [f\"{base}, variation {i+1}\" for i in range(n)]\n",
        "\n",
        "def generate_images_from_image(image_url: str, num_images=4):\n",
        "    print(f\"\\nGenerating {num_images} images for {image_url}\")\n",
        "\n",
        "    try:\n",
        "        # 1. Get image description\n",
        "        original_description = ask_about_image(image_url)\n",
        "        print(f\"Description: {original_description}\")\n",
        "\n",
        "        # 2. Generate diverse prompts\n",
        "        diffusion_prompts = generate_prompts(original_description, num_images)\n",
        "        print(\"Generated prompts:\")\n",
        "        for i, p in enumerate(diffusion_prompts, 1):\n",
        "            print(f\"{i}. {p}\")\n",
        "\n",
        "        # 3. Generate images\n",
        "        generated_images = []\n",
        "        for prompt in diffusion_prompts:\n",
        "            try:\n",
        "                result = sd_pipe(prompt, num_inference_steps=30)\n",
        "                generated_images.append(result.images[0])\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating image: {e}\")\n",
        "                generated_images.append(Image.new('RGB', (1024, 1024), color='grey'))\n",
        "\n",
        "        # 4. Display results\n",
        "        plot_imgs(generated_images)\n",
        "        return generated_images, diffusion_prompts, original_description\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Pipeline failed: {e}\")\n",
        "        return [], [], \"\"\n",
        "\n",
        "# Test cases\n",
        "test_images = [\n",
        "    \"imgs/agent-overview.png\",\n",
        "    \"imgs/multimodal.png\",\n",
        "    \"img-files/tree-frog.jpg\",\n",
        "    \"img-files/paint-cat.jpg\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "for img_url in test_images:\n",
        "    results.append(generate_images_from_image(img_url))"
      ],
      "metadata": {
        "id": "d2QDSGYZvRZW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}